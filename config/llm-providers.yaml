# Unified LLM Provider Configuration
# Merged from model-tiers.yaml + dmr-config.yaml with new providers
# Version: 1.0

# Available providers and their models per tier
providers:
  # Subscription providers (zero cost)
  claude-code:
    cliCommand: "claude"
    timeout: 120000
    models:
      fast: "sonnet"
      standard: "sonnet"
      premium: "opus"
    quotaTracking:
      enabled: true
      softLimitPerHour: 100  # Conservative estimate

  copilot:
    cliCommand: "copilot-cli"
    timeout: 120000
    models:
      fast: "claude-haiku-4.5"        # Benchmarked: 5s sequential, 0.77s @10 parallel
      standard: "claude-sonnet-4.5"
      premium: "claude-opus-4.6"
    quotaTracking:
      enabled: true
      softLimitPerHour: 100

  # API providers (per-token cost)
  groq:
    apiKeyEnvVar: GROQ_API_KEY
    fast: "llama-3.1-8b-instant"        # ~$0.05/M tokens, 750 tok/s
    standard: "llama-3.3-70b-versatile"  # ~$0.59/M tokens, 275 tok/s
    premium: "openai/gpt-oss-120b"      # 120B param, available on Groq

  anthropic:
    apiKeyEnvVar: ANTHROPIC_API_KEY
    fast: "claude-haiku-4-5"            # $1/$5 per MTok, fastest
    standard: "claude-sonnet-4-5"       # $3/$15 per MTok, fast + intelligent
    premium: "claude-opus-4-6"          # $5/$25 per MTok, most intelligent

  openai:
    apiKeyEnvVar: OPENAI_API_KEY
    fast: "gpt-4.1-mini"               # affordable small model
    standard: "gpt-4.1"                # latest standard model
    premium: "o4-mini"                  # reasoning model

  gemini:
    apiKeyEnvVar: GOOGLE_API_KEY
    fast: "gemini-2.5-flash"            # fast, cost-effective
    standard: "gemini-2.5-flash"        # good balance
    premium: "gemini-2.5-pro"           # deep reasoning

  github-models:
    apiKeyEnvVar: GITHUB_TOKEN
    baseUrl: "https://models.github.ai/inference/v1"
    fast: "gpt-4.1-mini"
    standard: "gpt-4.1"
    premium: "o4-mini"

# Default provider priority per tier
# Copilot first â€” scales beautifully with parallelism (0.77s effective @10 concurrent)
# Batch agents already use Promise.all, so copilot as primary unlocks peak throughput
# Groq second as fast API fallback, then claude-code, then paid APIs
provider_priority:
  fast: ["copilot", "groq", "claude-code", "anthropic", "openai", "gemini", "github-models"]
  standard: ["copilot", "groq", "claude-code", "anthropic", "openai", "gemini", "github-models"]
  premium: ["copilot", "groq", "claude-code", "anthropic", "openai", "gemini", "github-models"]

# Task-level provider priority overrides
# Overrides the tier-based priority for specific tasks that need different routing
# Use case: copilot proxy times out (120s) on large prompts; route to groq first
task_provider_priority:
  semantic_code_analysis: ["groq", "copilot", "claude-code", "anthropic", "openai"]
  ontology_classification: ["groq", "copilot", "claude-code", "anthropic", "openai"]

# Task-to-tier mapping
task_tiers:
  # Tier 1: Fast - Simple extraction and parsing
  fast:
    - git_file_extraction
    - commit_message_parsing
    - file_pattern_matching
    - basic_classification
    - documentation_file_scanning

  # Tier 2: Standard - Most semantic analysis
  standard:
    - git_history_analysis
    - vibe_history_analysis
    - semantic_code_analysis
    - documentation_linking
    - web_search_summarization
    - ontology_classification
    - content_validation
    - deduplication_similarity

  # Tier 3: Premium - Deep understanding required
  premium:
    - insight_generation
    - observation_generation
    - pattern_recognition
    - quality_assurance_review
    - deep_code_analysis
    - entity_significance_scoring

# Agent-level overrides (maps agent_id to tier)
agent_overrides:
  insight_generation: premium
  observation_generation: premium
  quality_assurance: premium
  semantic_analysis: standard
  git_history: standard
  vibe_history: standard
  ontology_classification: standard
  content_validation: standard
  batch_scheduler: fast
  batch_checkpoint_manager: fast
  kg_operators: standard

# Tree-KG Operator-specific tier assignments
operator_tiers:
  conv: premium
  aggr: standard
  embed: fast
  dedup: standard
  pred: premium
  merge: standard

# Batch workflow task mappings
batch_task_tiers:
  plan_batches: fast
  extract_batch_commits: fast
  extract_batch_sessions: fast
  batch_semantic_analysis: premium
  operator_conv: premium
  operator_aggr: standard
  operator_embed: fast
  operator_dedup: standard
  operator_pred: premium
  operator_merge: standard
  batch_qa: premium
  save_batch_checkpoint: fast
  final_persist: fast
  final_dedup: standard
  final_validation: standard

# Docker Model Runner (DMR) configuration
dmr:
  port: ${DMR_PORT:-12434}
  host: ${DMR_HOST:-localhost}
  baseUrl: http://${DMR_HOST:-localhost}:${DMR_PORT:-12434}/engines/v1
  defaultModel: ai/llama3.2
  modelOverrides:
    git_history: ai/llama3.2:3B-Q4_K_M
    vibe_history: ai/llama3.2:3B-Q4_K_M
    web_search: ai/llama3.2:3B-Q4_K_M
    documentation_linker: ai/llama3.2:3B-Q4_K_M
    semantic_analysis: ai/qwen2.5-coder:7B-Q4_K_M
    ontology_classification: ai/qwen2.5-coder:7B-Q4_K_M
    content_validation: ai/qwen2.5-coder:7B-Q4_K_M
    insight_generation: ai/llama3.2
    observation_generation: ai/llama3.2
    quality_assurance: ai/llama3.2
    kg_operators: ai/qwen2.5-coder:7B-Q4_K_M
  timeout: 120000
  maxTokens: 4096
  temperature: 0.7
  connection:
    maxRetries: 3
    retryDelay: 1000
    healthCheckInterval: 30000

# Cost tracking thresholds (per workflow run)
cost_limits:
  budget_mode: 0.05
  standard_mode: 0.50
  quality_mode: 2.00

# Batch-specific cost limits
batch_cost_limits:
  max_tokens_per_batch: 500000
  max_cost_per_batch_usd: 1.00
  fallback_on_quota: true
  total_budget_usd: 50.00

# Cache configuration
cache:
  maxSize: 1000
  ttlMs: 3600000  # 1 hour

# Circuit breaker configuration
circuit_breaker:
  threshold: 5
  resetTimeoutMs: 60000  # 1 minute

# Environment variable overrides
# Set these to force specific tiers:
#   SEMANTIC_ANALYSIS_TIER=premium  - All tasks use premium
#   SEMANTIC_ANALYSIS_PROVIDER=anthropic - Force specific provider
#   SEMANTIC_ANALYSIS_COST_MODE=budget - Use fast tier everywhere
