#!/bin/bash
# UKB (Update Knowledge Base) - Intelligent session insight capture
# Version 3.0 - Enhanced with interactive mode, significance ranking, and deep learning capture

set -euo pipefail

# Incremental processing state management
PROCESSING_STATE_FILE="$HOME/.ukb-processing-state.json"

# Initialize or load processing state
init_processing_state() {
    local project="$1"
    
    if [[ ! -f "$PROCESSING_STATE_FILE" ]]; then
        cat > "$PROCESSING_STATE_FILE" << EOF
{
  "projects": {},
  "global": {
    "last_updated": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "version": "3.0.0"
  }
}
EOF
    fi
    
    # Ensure project entry exists
    jq --arg proj "$project" \
       '.projects[$proj] //= {
           "last_commit_analyzed": null,
           "specstory_files_analyzed": [],
           "last_analysis": null,
           "schema_version": "2.0.0"
       }' \
       "$PROCESSING_STATE_FILE" > "$PROCESSING_STATE_FILE.tmp" && \
       mv "$PROCESSING_STATE_FILE.tmp" "$PROCESSING_STATE_FILE"
}

# Get last analyzed commit for project
get_last_analyzed_commit() {
    local project="$1"
    jq -r --arg proj "$project" '.projects[$proj].last_commit_analyzed // "never"' "$PROCESSING_STATE_FILE" 2>/dev/null || echo "never"
}

# Update last analyzed commit
update_last_analyzed_commit() {
    local project="$1"
    local commit_hash="$2"
    
    jq --arg proj "$project" \
       --arg commit "$commit_hash" \
       --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
       '.projects[$proj].last_commit_analyzed = $commit |
        .projects[$proj].last_analysis = $timestamp |
        .global.last_updated = $timestamp' \
       "$PROCESSING_STATE_FILE" > "$PROCESSING_STATE_FILE.tmp" && \
       mv "$PROCESSING_STATE_FILE.tmp" "$PROCESSING_STATE_FILE"
}

# Check if specstory file was already analyzed
is_specstory_analyzed() {
    local project="$1"
    local filename="$2"
    
    local count
    count=$(jq --arg proj "$project" --arg file "$filename" \
        '.projects[$proj].specstory_files_analyzed // [] | map(select(. == $file)) | length' \
        "$PROCESSING_STATE_FILE" 2>/dev/null || echo "0")
    [[ "$count" -gt 0 ]]
}

# Mark specstory file as analyzed
mark_specstory_analyzed() {
    local project="$1"
    local filename="$2"
    
    jq --arg proj "$project" \
       --arg file "$filename" \
       --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
       '.projects[$proj].specstory_files_analyzed |= (. + [$file] | unique) |
        .projects[$proj].last_analysis = $timestamp |
        .global.last_updated = $timestamp' \
       "$PROCESSING_STATE_FILE" > "$PROCESSING_STATE_FILE.tmp" && \
       mv "$PROCESSING_STATE_FILE.tmp" "$PROCESSING_STATE_FILE"
}

# Check if schema migration is needed
needs_schema_migration() {
    local current_version
    current_version=$(jq -r '.metadata.schema_version // "1.0.0"' "$SHARED_MEMORY" 2>/dev/null || echo "1.0.0")
    [[ "$current_version" != "2.0.0" ]]
}

# Configuration - Dynamic repo detection
# Find the repository root by looking for characteristic files/directories
find_repo_root() {
    local current_dir="$PWD"
    while [[ "$current_dir" != "/" ]]; do
        if [[ -f "$current_dir/shared-memory.json" ]] && [[ -d "$current_dir/knowledge-management" ]]; then
            echo "$current_dir"
            return 0
        fi
        current_dir="$(dirname "$current_dir")"
    done
    
    # Fallback: check if we're in a subdirectory of the expected location
    if [[ -f "/Users/q284340/Agentic/coding/shared-memory.json" ]]; then
        echo "/Users/q284340/Agentic/coding"
        return 0
    fi
    
    # Last resort: use current directory if it has the right structure  
    if [[ -f "shared-memory.json" ]] && [[ -d "knowledge-management" ]]; then
        echo "$PWD"
        return 0
    fi
    
    echo "/Users/q284340/Agentic/coding"  # Default fallback
}

CLAUDE_REPO="$(find_repo_root)"
SHARED_MEMORY="$CLAUDE_REPO/shared-memory.json"
INSIGHTS_DIR="$CLAUDE_REPO/knowledge-management/insights"
RELATIONS_DIR="$CLAUDE_REPO/knowledge-management/relations"
CLAUDE_CONVERSATION_LOG="/tmp/claude-conversation-latest.log"
SPECSTORY_DIR="$CLAUDE_REPO/.specstory/history"
TMP_DIR="/tmp/ukb-$$"
LOG_FILE="/tmp/ukb-session-$$.log"

# Ensure directories exist
mkdir -p "$INSIGHTS_DIR" "$RELATIONS_DIR" "$TMP_DIR"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Mode flags
INTERACTIVE_MODE=false
AUTO_MODE=false
AGENT_MODE=false
UPGRADE_MODE=false
FORCE_REPROCESS=false
FULL_HISTORY_MODE=false
HISTORY_DEPTH=""

# Get significance score for category
get_significance_score() {
    local category="$1"
    case "$category" in
        "architecture") echo 10 ;;
        "state-management") echo 9 ;;
        "refactoring") echo 8 ;;
        "design-pattern") echo 8 ;;
        "performance-optimization") echo 7 ;;
        "algorithm") echo 7 ;;
        "debugging-technique") echo 6 ;;
        "feature") echo 5 ;;
        "bug-fix") echo 3 ;;
        "documentation") echo 2 ;;
        "style") echo 1 ;;
        *) echo 5 ;;  # Default
    esac
}

# Logging function
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" >> "$LOG_FILE"
}

# URL validation function
validate_url() {
    local url="$1"
    local timeout=10
    
    # Check URL format first
    if [[ ! "$url" =~ ^https?:// ]]; then
        return 1
    fi
    
    # Test URL accessibility
    local status_code
    status_code=$(curl -s -o /dev/null -w "%{http_code}" --max-time "$timeout" "$url" 2>/dev/null || echo "000")
    
    # Accept 200-399 status codes (including redirects)
    if [[ "$status_code" =~ ^[23][0-9][0-9]$ ]]; then
        return 0
    else
        log "URL validation failed: $url (status: $status_code)"
        return 1
    fi
}

# Validate and filter URLs
validate_urls() {
    local urls_input="$1"
    local valid_urls=()
    
    if [[ -z "$urls_input" ]]; then
        echo ""
        return
    fi
    
    echo -e "${BLUE}üîó Validating reference URLs...${NC}"
    
    # Split URLs by comma and validate each
    IFS=',' read -ra urls <<< "$urls_input"
    for url in "${urls[@]}"; do
        # Trim whitespace
        url=$(echo "$url" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
        
        if [[ -n "$url" ]]; then
            echo -n "  Validating $url... "
            if validate_url "$url"; then
                valid_urls+=("$url")
                echo -e "${GREEN}‚úì${NC}"
            else
                echo -e "${RED}‚úó (invalid/unreachable)${NC}"
            fi
        fi
    done
    
    # Return comma-separated valid URLs
    if [[ ${#valid_urls[@]} -gt 0 ]]; then
        printf "%s," "${valid_urls[@]}" | sed 's/,$//'
    else
        echo ""
    fi
}

# Error handling
error_exit() {
    echo -e "${RED}ERROR: $1${NC}" >&2
    cleanup
    exit 1
}

# Cleanup function
cleanup() {
    rm -rf "$TMP_DIR" 2>/dev/null || true
}

# Trap cleanup on exit
trap cleanup EXIT

# Parse command line arguments
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --interactive|-i)
                INTERACTIVE_MODE=true
                shift
                ;;
            --auto|-a)
                AUTO_MODE=true
                shift
                ;;
            --agent|-g)
                AGENT_MODE=true
                shift
                ;;
            --upgrade)
                # Upgrade existing entities to enhanced schema format
                UPGRADE_MODE=true
                shift
                ;;
            --force-reprocess)
                # Force reprocessing of all files (ignore incremental state)
                FORCE_REPROCESS=true
                shift
                ;;
            --full-history)
                # Analyze entire git history for comprehensive codebase understanding
                FULL_HISTORY_MODE=true
                shift
                ;;
            --history-depth)
                # Specify how many commits to analyze (default: all if --full-history)
                if [[ -n "$2" ]] && [[ "$2" =~ ^[0-9]+$ ]]; then
                    HISTORY_DEPTH="$2"
                    shift 2
                else
                    error_exit "Invalid history depth. Must be a number."
                fi
                ;;
            --agent-complete)
                # Complete agent analysis with provided directory
                if [[ -n "$2" ]] && [[ -d "$2" ]]; then
                    complete_agent_analysis "$2"
                    exit 0
                else
                    error_exit "Invalid agent session directory"
                fi
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            *)
                echo "Unknown option: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # Default to auto mode if nothing specified (unless upgrade mode)
    if [[ "$INTERACTIVE_MODE" == false ]] && [[ "$AUTO_MODE" == false ]] && [[ "$AGENT_MODE" == false ]] && [[ "$UPGRADE_MODE" == false ]]; then
        AUTO_MODE=true
    fi
    
    # Full history mode implies auto mode
    if [[ "$FULL_HISTORY_MODE" == true ]]; then
        AUTO_MODE=true
    fi
}

# Show help
show_help() {
    echo -e "${CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
    echo -e "${CYAN}‚ïë          UKB - Update Knowledge Base v3.0                    ‚ïë${NC}"
    echo -e "${CYAN}‚ïë     Intelligent Session Insight Capture & Learning           ‚ïë${NC}"
    echo -e "${CYAN}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
    echo ""
    echo -e "${YELLOW}Usage:${NC} ukb [OPTIONS]"
    echo ""
    echo -e "${YELLOW}Options:${NC}"
    echo "  --auto, -a        Automatic mode (analyzes session & Claude conversations)"
    echo "  --interactive, -i Interactive mode (prompts for deep insights)"
    echo "  --agent, -g       Agent mode (semantic analysis within coding agent)"
    echo "  --upgrade         Upgrade existing entities to enhanced schema format"
    echo "  --force-reprocess Force reprocessing of all files (ignore incremental state)"
    echo "  --full-history    Analyze entire git history for comprehensive understanding"
    echo "  --history-depth N Analyze last N commits (use with --full-history for limit)"
    echo "  --help, -h        Show this help message"
    echo ""
    echo -e "${YELLOW}Features:${NC}"
    echo "  ‚Ä¢ Captures deep architectural decisions and thought processes"
    echo "  ‚Ä¢ Ranks insights by significance (1-10 scale)"
    echo "  ‚Ä¢ Interactive mode for detailed learning capture"
    echo "  ‚Ä¢ Auto mode analyzes Claude conversations & code changes"
    echo "  ‚Ä¢ Integrates .specstory/history for AI-assisted insights"
    echo "  ‚Ä¢ Creates transferable patterns from high-significance learnings"
    echo "  ‚Ä¢ Distinguishes between routine fixes and profound learnings"
    echo "  ‚Ä¢ Tracks problem-solving journeys and design rationales"
    echo ""
    echo -e "${BLUE}üí° Use interactive mode after major architecture decisions${NC}"
    echo -e "${BLUE}üí° Auto mode great for quick session summaries${NC}"
    echo -e "${BLUE}üí° Agent mode enables semantic analysis within coding agents${NC}"
    echo -e "${BLUE}üí° Full history mode for comprehensive codebase understanding${NC}"
    echo ""
    echo -e "${YELLOW}Full History Analysis Examples:${NC}"
    echo "  ukb --full-history              # Analyze entire git history"
    echo "  ukb --full-history --interactive  # Deep analysis with manual insights"
    echo "  ukb --history-depth 100         # Analyze last 100 commits"
    echo "  ukb --full-history --force-reprocess  # Re-analyze everything"
    echo ""
    echo -e "${YELLOW}Agent Mode Usage:${NC}"
    echo "  1. Run 'ukb --agent' from within Claude/CoPilot"
    echo "  2. Agent analyzes commits, specstory history, and code"
    echo "  3. Agent extracts and documents transferable patterns"
    echo "  4. Complete with 'ukb --agent-complete <session-dir>'"
}

# Calculate significance score based on content
calculate_significance() {
    local content="$1"
    local category="$2"
    local score=5  # Default middle score
    
    # Start with category base score
    score=$(get_significance_score "$category")
    
    # Adjust based on content keywords
    local profound_keywords=(
        "fundamental" "architecture" "redesign" "paradigm" "pattern"
        "principle" "scalability" "maintainability" "decoupling"
        "abstraction" "refactor" "state management" "redux" "context"
        "performance breakthrough" "algorithm" "data structure"
        "system design" "microservice" "monolith" "migration"
    )
    
    for keyword in "${profound_keywords[@]}"; do
        if [[ "$(echo "$content" | tr '[:upper:]' '[:lower:]')" =~ $keyword ]]; then
            ((score++))
            [[ $score -gt 10 ]] && score=10
        fi
    done
    
    echo "$score"
}

# Analyze Claude conversation for insights
analyze_claude_conversation() {
    local insights_file="$1"
    
    log "Analyzing Claude conversation for deep insights..."
    
    # Check if conversation log exists (this would be populated by Claude Code)
    if [[ ! -f "$CLAUDE_CONVERSATION_LOG" ]]; then
        log "No Claude conversation log found"
        return
    fi
    
    # Extract key learning moments from conversation
    # This is a simplified version - in reality, Claude would analyze the full conversation
    local conversation_insights="$TMP_DIR/conversation_insights.json"
    
    cat > "$conversation_insights" << 'EOF'
{
  "insights": [
    {
      "type": "architecture",
      "problem": "Knowledge management system was capturing only surface-level commit data",
      "solution": "Redesigned to capture deep insights, thought processes, and architectural decisions",
      "rationale": "Commits alone miss the 'why' behind decisions and learning journey",
      "learnings": [
        "Explicit knowledge capture beats implicit extraction",
        "Ranking insights by significance helps focus on profound learnings",
        "Interactive and auto modes serve different use cases"
      ],
      "significance": 9
    }
  ]
}
EOF
    
    # Merge conversation insights into main insights file
    jq -s '.[0] * .[1]' "$insights_file" "$conversation_insights" > "$insights_file.tmp" && \
        mv "$insights_file.tmp" "$insights_file"
}

# Analyze .specstory history for transferable insights
analyze_specstory_history() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Analyzing .specstory history for transferable insights..."
    
    # Check if .specstory/history directory exists
    if [[ ! -d "$SPECSTORY_DIR" ]]; then
        log "No .specstory/history directory found"
        return
    fi
    
    # Get recent conversation files (last 7 days)
    local recent_files
    recent_files=$(find "$SPECSTORY_DIR" -name "*.md" -type f -mtime -7 2>/dev/null | sort -r)
    
    if [[ -z "$recent_files" ]]; then
        log "No recent .specstory files found"
        echo "  No recent .specstory files found in last 7 days"
        return
    fi
    
    log "Found $(echo "$recent_files" | wc -l) recent .specstory files"
    
    local insight_count=0
    
    # Process each conversation file (skip already analyzed ones)
    while IFS= read -r specstory_file; do
        [[ -z "$specstory_file" ]] && continue
        
        local filename=$(basename "$specstory_file")
        
        # Skip if already analyzed (unless force reprocess)
        if [[ "$FORCE_REPROCESS" != true ]] && is_specstory_analyzed "$project" "$filename"; then
            log "Skipping already analyzed file: $filename"
            continue
        fi
        
        log "Processing NEW file: $filename"
        
        # Extract key patterns from conversation
        # Look for problem-solving patterns, architectural decisions, debugging techniques
        
        # Extract user questions that indicate learning opportunities
        local user_questions
        user_questions=$(grep -A 5 "^## User" "$specstory_file" 2>/dev/null | grep -v "^--" | grep -v "^## User" || true)
        
        # Extract assistant responses that contain solutions
        local assistant_solutions
        assistant_solutions=$(grep -A 20 "^## Assistant" "$specstory_file" 2>/dev/null | grep -v "^--" | grep -v "^## Assistant" || true)
        
        # Look for specific patterns indicating transferable knowledge
        local transferable_patterns=()
        
        # Pattern 1: Architecture/Design discussions
        if echo "$assistant_solutions" | grep -qiE "(architecture|design pattern|refactor|restructure|modular|decoupl)"; then
            transferable_patterns+=("architecture")
        fi
        
        # Pattern 2: Debugging techniques
        if echo "$assistant_solutions" | grep -qiE "(debug|troubleshoot|diagnose|root cause|stack trace)"; then
            transferable_patterns+=("debugging")
        fi
        
        # Pattern 3: Performance optimizations
        if echo "$assistant_solutions" | grep -qiE "(performance|optimize|speed up|memory|efficient|cache)"; then
            transferable_patterns+=("performance")
        fi
        
        # Pattern 4: State management patterns
        if echo "$assistant_solutions" | grep -qiE "(state management|redux|context|global state|store)"; then
            transferable_patterns+=("state-management")
        fi
        
        # Pattern 5: Error handling patterns
        if echo "$assistant_solutions" | grep -qiE "(error handling|exception|try.catch|error boundary|fallback)"; then
            transferable_patterns+=("error-handling")
        fi
        
        # Pattern 6: Testing strategies
        if echo "$assistant_solutions" | grep -qiE "(test|jest|pytest|unit test|integration|mock)"; then
            transferable_patterns+=("testing")
        fi
        
        # Extract meaningful insights only if we find specific, actionable content
        for pattern in "${transferable_patterns[@]}"; do
            # Extract domain-specific context for meaningful naming
            local domain_context=""
            local specific_insight=""
            
            # Look for specific technologies, frameworks, or domain concepts
            domain_context=$(echo "$assistant_solutions" | grep -oiE "(react|redux|mcp|claude|three\.js|animation|state|performance|logging|memory|sync|api|database|auth|testing|webpack|babel|typescript|javascript|python|rust|docker|kubernetes|aws|azure|gcp)" | head -3 | tr '\n' ' ' | sed 's/ $//')
            
            # Extract the actual problem/solution pair
            local problem_line=""
            local solution_line=""
            problem_line=$(echo "$user_questions" | grep -iE "(issue|problem|error|fail|bug|challenge|stuck|help)" | head -1 | cut -c1-100)
            solution_line=$(echo "$assistant_solutions" | grep -iE "(solution|fix|resolve|implement|create|add|use)" | head -1 | cut -c1-100)
            
            # Only create insight if we have meaningful domain context
            if [[ -n "$domain_context" ]] && [[ -n "$problem_line" ]] && [[ -n "$solution_line" ]]; then
                ((insight_count++))
                
                # Create specific, meaningful name based on domain and pattern
                local insight_name
                local primary_tech=$(echo "$domain_context" | awk '{print $1}' | tr '[:lower:]' '[:upper:]')
                
                case "$pattern" in
                    "architecture")
                        insight_name="${primary_tech}ArchitecturalDesignPattern"
                        ;;
                    "state-management")
                        insight_name="${primary_tech}StateManagementPattern"
                        ;;
                    "performance")
                        insight_name="${primary_tech}PerformanceOptimizationPattern"
                        ;;
                    "debugging")
                        insight_name="${primary_tech}DebuggingMethodologyPattern"
                        ;;
                    "error-handling")
                        insight_name="${primary_tech}ErrorHandlingPattern"
                        ;;
                    "testing")
                        insight_name="${primary_tech}TestingStrategyPattern"
                        ;;
                    *)
                        insight_name="${primary_tech}${pattern^}Pattern"
                        ;;
                esac
                
                # Ensure name is not too generic
                if [[ "$insight_name" =~ ^(Complex|Generic|Basic|Simple) ]] || [[ ${#insight_name} -lt 10 ]]; then
                    log "Skipping generic insight name: $insight_name"
                    continue
                fi
                
                # Higher threshold for significance - only capture truly valuable insights
                local significance=7  # Default for specstory patterns
                case "$pattern" in
                    "architecture") significance=9 ;;
                    "state-management") significance=8 ;;
                    "performance") significance=8 ;;
                    *) significance=7 ;;
                esac
                
                # Only proceed if high enough significance
                if [[ $significance -lt 7 ]]; then
                    continue
                fi
                
                local timestamp
                timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
                
                # Generate detailed insight page for this pattern
                local doc_filepath
                doc_filepath=$(generate_insight_page "$insight_name" "$problem_line" "$solution_line" "AI-assisted problem solving using $domain_context" "$domain_context" "Projects using $domain_context facing similar $pattern challenges" "" "" "$project")
                local doc_filename=$(basename "$doc_filepath")
                local doc_url="http://localhost:8080/knowledge-management/insights/${doc_filename}"
                
                # Create structured insight with proper problem/solution format
                jq --arg name "$insight_name" \
                   --arg type "TransferablePattern" \
                   --arg pattern "$pattern" \
                   --arg problem "$problem_line" \
                   --arg solution "$solution_line" \
                   --arg domain "$domain_context" \
                   --arg file "$(basename "$specstory_file")" \
                   --arg proj "$project" \
                   --arg lang "$language" \
                   --arg sig "$significance" \
                   --arg timestamp "$timestamp" \
                   --arg docurl "$doc_url" \
                   '.insights += [{
                       "type": "entity",
                       "name": $name,
                       "entityType": $type,
                       "problem": $problem,
                       "solution": $solution,
                       "approach": "AI-assisted problem solving using \($domain)",
                       "applicability": "Projects using \($domain) facing similar \($pattern) challenges",
                       "technologies": ($domain | split(" ") | map(select(length > 0))),
                       "author": "ai-assisted",
                       "project": $proj,
                       "observations": [
                           "Problem: \($problem)",
                           "Solution: \($solution)",
                           "Domain: \($domain)",
                           "Pattern type: \($pattern)",
                           "Extracted from: \($file)",
                           "Details: \($docurl)",
                           "Significance: \($sig)/10"
                       ],
                       "significance": ($sig | tonumber),
                       "created": $timestamp,
                       "metadata": {
                           "pattern_type": $pattern,
                           "source_file": $file,
                           "source": "specstory-analysis",
                           "domain_context": $domain
                       }
                   }]' "$insights_file" > "$insights_file.tmp" && mv "$insights_file.tmp" "$insights_file"
                
                log "Created meaningful insight: $insight_name (domain: $domain_context)"
                
                # Mark file as analyzed after successful processing
                mark_specstory_analyzed "$project" "$filename"
            else
                log "Skipping pattern '$pattern' - insufficient meaningful context"
            fi
        done
        
        # If no meaningful insights were extracted, still mark as analyzed to avoid reprocessing
        if [[ $insight_count -eq 0 ]]; then
            mark_specstory_analyzed "$project" "$filename"
            log "No insights extracted, but marked as analyzed: $filename"
        fi
        
    done <<< "$recent_files"
    
    if [[ $insight_count -gt 0 ]]; then
        log "Extracted $insight_count transferable insights from .specstory history"
        
        # Create transferable patterns from high-significance insights
        create_transferable_patterns "$insights_file" "$project"
    fi
}

# Create transferable patterns from specstory insights - DISABLED to prevent generic patterns
create_transferable_patterns() {
    local insights_file="$1"
    local project="$2"
    
    log "CONSERVATIVE MODE: Skipping automatic transferable pattern creation"
    log "Use 'ukb --interactive' to manually capture high-value transferable patterns"
    
    # DISABLED: The automatic pattern creation was generating too many generic patterns
    # Only the improved analyze_specstory_history() function should create patterns now
    # This ensures all patterns have specific domain context and meaningful names
    
    return 0
}

# Generate detailed insight page
generate_insight_page() {
    local insight_name="$1"
    local problem="$2"
    local solution="$3"
    local approach="$4"
    local technologies="$5"
    local applicability="$6"
    local code_files="$7"
    local references="$8"
    local project="$9"
    
    # Create filename from insight name
    local filename=$(echo "$insight_name" | tr '[:upper:]' '[:lower:]' | sed 's/pattern$//' | sed 's/[^a-z0-9-]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
    filename="${filename}.md"
    local filepath="$INSIGHTS_DIR/$filename"
    
    # Generate the insight page
    cat > "$filepath" << EOF
# $insight_name

## Overview

**Problem:** $problem

**Solution:** $solution

**Approach:** $approach

## Applicability

$applicability

## Technologies

$(echo "$technologies" | tr ',' '\n' | sed 's/^/- /')

## Implementation Details

### Problem Context

$problem

### Solution Strategy

$solution

### Key Implementation Points

$approach

$(if [[ -n "$code_files" ]]; then
    echo "## Key Files"
    echo ""
    echo "$code_files" | tr ',' '\n' | sed 's/^/- /'
    echo ""
fi)

$(if [[ -n "$references" ]]; then
    echo "## References"
    echo ""
    echo "$references" | tr ',' '\n' | sed 's/^/- /'
    echo ""
fi)

## Project Context

**Origin Project:** $project

## Usage Guidelines

This pattern can be applied when:

1. You encounter similar problems in projects using the same technologies
2. The architectural challenges match the described context
3. The solution approach aligns with your project constraints

## Related Patterns

- Consider checking other patterns in the same technology stack
- Look for complementary architectural patterns in the knowledge base

---

*Generated by ukb (Update Knowledge Base) - $(date)*
EOF
    
    echo "$filepath"
}

# Interactive insight capture
capture_interactive_insight() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    echo -e "${CYAN}üéØ Interactive Insight Capture${NC}"
    echo -e "${CYAN}==============================${NC}"
    echo ""
    
    # Problem Context
    echo -e "${YELLOW}1. What problem or challenge did you face?${NC}"
    echo -e "${BLUE}(Describe the core issue, not just symptoms)${NC}"
    read -r -p "> " problem_context
    
    # Solution Approach
    echo -e "\n${YELLOW}2. How did you solve it?${NC}"
    echo -e "${BLUE}(Include approaches tried, what worked/didn't)${NC}"
    read -r -p "> " solution_approach
    
    # Design Rationale
    echo -e "\n${YELLOW}3. Why did you choose this solution?${NC}"
    echo -e "${BLUE}(Trade-offs, alternatives considered, constraints)${NC}"
    read -r -p "> " design_rationale
    
    # Key Learnings
    echo -e "\n${YELLOW}4. What did you learn?${NC}"
    echo -e "${BLUE}(Insights that will help in future projects)${NC}"
    read -r -p "> " key_learnings
    
    # Applicability
    echo -e "\n${YELLOW}5. Where else could this be applied?${NC}"
    echo -e "${BLUE}(What types of projects/situations would benefit)${NC}"
    read -r -p "> " applicability
    
    # Technologies
    echo -e "\n${YELLOW}6. Technologies involved (comma-separated):${NC}"
    echo -e "${BLUE}(e.g., React, TypeScript, Redis, Docker)${NC}"
    read -r -p "> " technologies_input
    
    # References
    echo -e "\n${YELLOW}7. Helpful references (URLs, comma-separated, optional):${NC}"
    echo -e "${BLUE}(Documentation, tutorials, repos that help understand this pattern)${NC}"
    read -r -p "> " references_input
    
    # Validate URLs if provided
    local validated_references=""
    if [[ -n "$references_input" ]]; then
        validated_references=$(validate_urls "$references_input")
        if [[ -z "$validated_references" ]]; then
            echo -e "${YELLOW}‚ö†Ô∏è  No valid URLs found, continuing without references${NC}"
        fi
    fi
    
    # Code files
    echo -e "\n${YELLOW}8. Key code files (comma-separated, optional):${NC}"
    echo -e "${BLUE}(Main files that implement this pattern)${NC}"
    read -r -p "> " code_files_input
    
    # Category
    echo -e "\n${YELLOW}9. Category:${NC}"
    echo "   1) Architecture Decision"
    echo "   2) Performance Optimization"
    echo "   3) State Management Pattern"
    echo "   4) Integration Pattern"
    echo "   5) Development Workflow"
    echo "   6) Security Pattern"
    echo "   7) Testing Strategy"
    echo "   8) Other"
    read -r -p "Select (1-8): " category_choice
    
    local category_map=(
        "" "architecture" "performance" "state-management" "integration"
        "workflow" "security" "testing" "general"
    )
    local category="general"
    if [[ "$category_choice" =~ ^[1-8]$ ]] && [[ -n "${category_map[$category_choice]}" ]]; then
        category="${category_map[$category_choice]}"
    fi
    
    # Process arrays with proper JSON escaping
    local technologies_array
    if [[ -n "$technologies_input" ]]; then
        # Split by comma, trim whitespace, and build JSON array properly
        technologies_array=$(echo "$technologies_input" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R -s 'split("\n") | map(select(length > 0))')
    else
        technologies_array="[]"
    fi
    
    local references_array
    if [[ -n "$validated_references" ]]; then
        references_array=$(echo "$validated_references" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R -s 'split("\n") | map(select(length > 0))')
    else
        references_array="[]"
    fi
    
    local code_files_array
    if [[ -n "$code_files_input" ]]; then
        code_files_array=$(echo "$code_files_input" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R -s 'split("\n") | map(select(length > 0))')
    else
        code_files_array="[]"
    fi
    
    # Calculate significance (7-9 range for interactive entries)
    local significance=8  # Default high significance for manually captured insights
    if [[ "$category" == "architecture" ]]; then
        significance=9
    elif [[ "$category" == "performance" || "$category" == "state-management" ]]; then
        significance=8
    else
        significance=7
    fi
    
    # Create meaningful pattern name
    local base_name=$(echo "$problem_context" | sed 's/[^a-zA-Z0-9 ]//g' | awk '{print $1$2$3}' | sed 's/ //g')
    local insight_name="${base_name}Pattern"
    [[ ${#insight_name} -lt 5 ]] && insight_name="${category}Pattern_${project}"
    
    local timestamp
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    local author
    author=$(whoami)
    
    # Generate detailed insight page
    local doc_filepath
    doc_filepath=$(generate_insight_page "$insight_name" "$problem_context" "$solution_approach" "$design_rationale" "$technologies_input" "$applicability" "$code_files_input" "$validated_references" "$project_name")
    local doc_filename=$(basename "$doc_filepath")
    local doc_link="knowledge-management/insights/${doc_filename}"
    local doc_url="http://localhost:8080/knowledge-management/insights/${doc_filename}"
    
    # Add to insights file with new structured format
    jq --arg name "$insight_name" \
       --arg type "TransferablePattern" \
       --arg problem "$problem_context" \
       --arg solution "$solution_approach" \
       --arg approach "$design_rationale" \
       --arg learnings "$key_learnings" \
       --arg applicability "$applicability" \
       --argjson technologies "$technologies_array" \
       --argjson references "$references_array" \
       --argjson code_files "$code_files_array" \
       --arg author "$author" \
       --arg proj "$project" \
       --arg lang "$language" \
       --arg sig "$significance" \
       --arg timestamp "$timestamp" \
       --arg doclink "$doc_link" \
       --arg docurl "$doc_url" \
       '.insights += [{
           "type": "entity",
           "name": $name,
           "entityType": $type,
           "problem": $problem,
           "solution": $solution,
           "approach": $approach,
           "applicability": $applicability,
           "technologies": $technologies,
           "references": $references,
           "author": $author,
           "project": $proj,
           "code_files": $code_files,
           "documentation_link": $doclink,
           "observations": ([
               "Problem: \($problem)",
               "Solution: \($solution)",
               "Approach: \($approach)",
               "Key learnings: \($learnings)",
               "Applicability: \($applicability)",
               "Details: \($docurl)",
               "Significance: \($sig)/10",
               "Created: \($timestamp)"
           ] + (if ($references | length) > 0 then ["References: " + ($references | join(", "))] else [] end)),
           "significance": ($sig | tonumber),
           "created": $timestamp
       }]' "$insights_file" > "$insights_file.tmp" && mv "$insights_file.tmp" "$insights_file"
    
    echo -e "\n${GREEN}‚úÖ Insight captured with significance: $significance/10${NC}"
}

# Enhanced automatic insight extraction - CONSERVATIVE MODE
extract_auto_insights() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Extracting insights in CONSERVATIVE mode - quality over quantity..."
    
    # Initialize insights file
    echo '{"insights": [], "entities": [], "relations": []}' > "$insights_file"
    
    # Choose analysis approach based on mode
    if [[ -f "$TMP_DIR/recent_commits.txt" ]]; then
        if [[ "$FULL_HISTORY_MODE" == true ]]; then
            # Comprehensive analysis for full understanding
            analyze_git_history_comprehensively "$insights_file" "$project" "$language"
        else
            # Conservative analysis for incremental updates
            analyze_commits_conservatively "$insights_file" "$project" "$language"
        fi
    fi
    
    # Skip automatic .specstory analysis to prevent noise
    # Only interactive mode or manual curation should add patterns
    
    log "Conservative analysis complete - focusing on high-value insights only"
}

# Agent mode: Semantic analysis from within coding agents
extract_agent_insights() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Agent mode: Preparing for semantic analysis..."
    
    # Initialize insights file
    echo '{"insights": [], "entities": [], "relations": []}' > "$insights_file"
    
    # Create analysis request for coding agent
    local analysis_request="$TMP_DIR/agent_analysis_request.json"
    
    # Gather available data sources
    local recent_commits=""
    if [[ -f "$TMP_DIR/recent_commits.txt" ]]; then
        recent_commits=$(cat "$TMP_DIR/recent_commits.txt" | head -20)
    fi
    
    # Find recent specstory files (excluding current session)
    local specstory_files=""
    if [[ -d "$SPECSTORY_DIR" ]]; then
        # Get files from last 7 days, excluding today to avoid current session
        specstory_files=$(find "$SPECSTORY_DIR" -name "*.md" -type f -mtime +0 -mtime -7 2>/dev/null | sort -r | head -10)
    fi
    
    # Find recent code changes
    local changed_files=""
    if command -v git >/dev/null 2>&1; then
        changed_files=$(git diff --name-only HEAD~10..HEAD 2>/dev/null | head -20)
    fi
    
    # Create semantic analysis request
    cat > "$analysis_request" << EOF
{
  "mode": "semantic_analysis",
  "project": "$project",
  "language": "$language",
  "sources": {
    "recent_commits": $(echo "$recent_commits" | jq -R -s '.'),
    "specstory_files": $(echo "$specstory_files" | jq -R -s 'split("\n") | map(select(length > 0))'),
    "changed_files": $(echo "$changed_files" | jq -R -s 'split("\n") | map(select(length > 0))')
  },
  "analysis_targets": [
    "architectural_patterns",
    "debugging_workflows",
    "performance_optimizations",
    "error_solutions",
    "tool_usage_patterns",
    "state_management",
    "integration_patterns",
    "testing_strategies"
  ],
  "output_format": "structured_insights"
}
EOF
    
    echo -e "${YELLOW}üìã Semantic Analysis Request Created${NC}"
    echo -e "${BLUE}The coding agent should analyze:${NC}"
    echo "  ‚Ä¢ Recent commits for architectural patterns"
    echo "  ‚Ä¢ Previous conversation files for problem-solution pairs"
    echo "  ‚Ä¢ Code changes for design patterns"
    echo "  ‚Ä¢ Tool usage sequences for workflow patterns"
    echo ""
    echo -e "${YELLOW}To complete the analysis, the agent should:${NC}"
    echo "  1. Read and analyze the specstory files listed"
    echo "  2. Extract transferable patterns and insights"
    echo "  3. Create structured entries in $insights_file"
    echo "  4. Run: ukb --agent-complete $TMP_DIR"
    echo ""
    echo -e "${CYAN}Analysis request saved to: $analysis_request${NC}"
    
    # Run semantic analysis scripts if available
    if command -v node >/dev/null 2>&1; then
        echo -e "${BLUE}üîß Running automated analysis scripts...${NC}"
        
        # Run conversation analysis
        if [[ -f "$CLAUDE_REPO/knowledge-management/scripts/analyze-conversations.js" ]]; then
            echo "  ‚Ä¢ Analyzing conversation patterns..."
            node "$CLAUDE_REPO/knowledge-management/scripts/analyze-conversations.js" "$TMP_DIR" 2>/dev/null || true
        fi
        
        # Run code analysis
        if [[ -f "$CLAUDE_REPO/knowledge-management/scripts/analyze-code.js" ]]; then
            echo "  ‚Ä¢ Analyzing code patterns..."
            node "$CLAUDE_REPO/knowledge-management/scripts/analyze-code.js" "$TMP_DIR" 2>/dev/null || true
        fi
        
        # Run reference enrichment
        if [[ -f "$CLAUDE_REPO/knowledge-management/scripts/enrich-references.js" ]]; then
            echo "  ‚Ä¢ Enriching with reference documentation..."
            node "$CLAUDE_REPO/knowledge-management/scripts/enrich-references.js" enrich "$insights_file" 2>/dev/null || true
        fi
        
        echo -e "${GREEN}‚úÖ Automated analysis completed${NC}"
        
        # Check if insights were generated
        if [[ -f "$insights_file" ]] && [[ $(jq '.insights | length' "$insights_file" 2>/dev/null || echo "0") -gt 0 ]]; then
            echo -e "${GREEN}üéØ Found insights from automated analysis - proceeding to completion${NC}"
            complete_agent_analysis "$TMP_DIR"
            return
        fi
    fi
    
    # Create placeholder for agent to fill
    echo -e "${GREEN}Waiting for agent to complete semantic analysis...${NC}"
    echo -e "${YELLOW}Agent should use the following structure for insights:${NC}"
    cat > "$TMP_DIR/insight_template.json" << 'EOF'
{
  "type": "entity",
  "name": "PatternName",
  "entityType": "TransferablePattern",
  "problem": "Clear problem description",
  "solution": "Solution approach",
  "approach": "Implementation details",
  "applicability": "Where this can be applied",
  "technologies": ["Tech1", "Tech2"],
  "code_files": ["file1.js", "file2.js"],
  "references": ["https://reference-url.com"],
  "observations": [
    "Key insight 1",
    "Key insight 2",
    "Performance impact: X% improvement",
    "Details: http://localhost:8080/knowledge-management/insights/pattern-name.md"
  ],
  "significance": 8,
  "metadata": {
    "source": "agent-analysis",
    "extracted_from": ["specstory", "commits", "code"]
  }
}
EOF
    
    # Save state for resumption
    echo "$TMP_DIR" > "$HOME/.ukb_agent_session"
    
    log "Agent analysis request prepared - waiting for semantic analysis completion"
}

# Complete agent analysis - called by coding agent after semantic analysis
complete_agent_analysis() {
    local session_dir="$1"
    local insights_file="$session_dir/insights.json"
    
    echo -e "${CYAN}üéØ Completing agent analysis...${NC}"
    
    # Verify insights file exists and has content
    if [[ ! -f "$insights_file" ]] || [[ $(jq '.insights | length' "$insights_file" 2>/dev/null || echo "0") -eq 0 ]]; then
        error_exit "No insights found in agent analysis. Please ensure insights were added to $insights_file"
    fi
    
    # Load session info
    local project main_language
    if [[ -f "$session_dir/session_analysis.json" ]]; then
        project=$(jq -r '.project' "$session_dir/session_analysis.json")
        main_language=$(jq -r '.main_language' "$session_dir/session_analysis.json")
    else
        project=$(basename "$PWD")
        main_language="unknown"
    fi
    
    # Process the insights
    echo -e "${GREEN}‚úÖ Found $(jq '.insights | length' "$insights_file") insights from agent analysis${NC}"
    
    # Create MCP entities and relationships
    create_mcp_entities "$insights_file"
    create_project_relationships
    
    # Process MCP entities if any were created
    process_mcp_entities
    
    # Update metadata
    local timestamp contributor
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    contributor="agent-$(whoami)"
    
    jq --arg contributor "$contributor" \
       --arg timestamp "$timestamp" \
       --arg mode "agent-semantic" \
       '.metadata.contributors |= (. + [$contributor] | unique) |
        .metadata.last_updated = $timestamp |
        .metadata.last_mode = $mode' \
       "$SHARED_MEMORY" > "$SHARED_MEMORY.tmp" && mv "$SHARED_MEMORY.tmp" "$SHARED_MEMORY"
    
    # Show summary
    show_insight_summary
    
    # Final stats
    local entity_count relation_count
    entity_count=$(jq '.entities | length' "$SHARED_MEMORY")
    relation_count=$(jq '.relations | length' "$SHARED_MEMORY")
    
    echo -e "\n${GREEN}‚úÖ Agent analysis completed successfully!${NC}"
    echo -e "${GREEN}üìä Total entities: $entity_count${NC}"
    echo -e "${GREEN}üîó Total relations: $relation_count${NC}"
    echo -e "${GREEN}üíæ Shared memory: $SHARED_MEMORY${NC}"
    
    # Cleanup session marker
    rm -f "$HOME/.ukb_agent_session"
    
    log "Agent analysis completed successfully"
}

# Comprehensive git history analysis for full codebase understanding
analyze_git_history_comprehensively() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Comprehensive git history analysis for full codebase understanding..."
    
    if [[ ! -f "$TMP_DIR/recent_commits.txt" ]]; then
        log "No commits to analyze"
        return
    fi
    
    local total_commits
    total_commits=$(wc -l < "$TMP_DIR/recent_commits.txt" 2>/dev/null || echo 0)
    
    if [[ $total_commits -eq 0 ]]; then
        log "No commits found to analyze"
        return
    fi
    
    echo "üîç Analyzing $total_commits commits for comprehensive understanding..."
    
    # Analyze commits in chronological order (oldest first)
    local architectural_patterns=()
    local feature_evolution=()
    local refactoring_events=()
    local technology_adoptions=()
    local performance_improvements=()
    
    # Process commits and categorize them
    local commit_count=0
    while IFS='|' read -r hash author date message; do
        [[ -z "$message" ]] && continue
        ((commit_count++))
        
        # Show progress for large histories
        if [[ $commit_count -eq 1 ]] || [[ $((commit_count % 50)) -eq 0 ]] || [[ $commit_count -eq $total_commits ]]; then
            echo "  Progress: $commit_count/$total_commits commits processed..."
        fi
        
        # Categorize commit types
        local commit_category=""
        local significance=5
        
        # Architecture & Design
        if [[ "$message" =~ (architect|design|structure|pattern|refactor|restructure|redesign) ]]; then
            commit_category="architecture"
            significance=8
            architectural_patterns+=("$hash:$message")
        # New Features & Capabilities
        elif [[ "$message" =~ ^(feat|add|implement|introduce):.*|^Add.*|^Implement.* ]]; then
            commit_category="feature"
            significance=6
            feature_evolution+=("$hash:$message")
        # Technology & Dependencies
        elif [[ "$message" =~ (upgrade|update|migrate|dependency|package|library|framework) ]]; then
            commit_category="technology"
            significance=7
            technology_adoptions+=("$hash:$message")
        # Performance & Optimization
        elif [[ "$message" =~ (perf|performance|optimize|speed|cache|memory|efficient) ]]; then
            commit_category="performance"
            significance=7
            performance_improvements+=("$hash:$message")
        # Major Refactoring
        elif [[ "$message" =~ (refactor|cleanup|reorganize|consolidate) ]]; then
            commit_category="refactoring"
            significance=6
            refactoring_events+=("$hash:$message")
        # Configuration & Setup
        elif [[ "$message" =~ (config|setup|init|install|deploy) ]]; then
            commit_category="configuration"
            significance=4
        # Bug fixes (lower priority for history analysis)
        elif [[ "$message" =~ ^(fix|bug):.*|^Fix.* ]]; then
            commit_category="bugfix"
            significance=3
        # Documentation (lowest priority)
        elif [[ "$message" =~ (doc|readme|comment) ]]; then
            commit_category="documentation"
            significance=2
        else
            commit_category="general"
            significance=4
        fi
        
        # Create insights for high-significance commits
        if [[ $significance -ge 6 ]] && [[ "$commit_category" != "general" ]]; then
            create_historical_insight "$hash" "$message" "$commit_category" "$significance" "$insights_file" "$project" "$language" "$author" "$date"
        fi
        
    done < <(tac "$TMP_DIR/recent_commits.txt")  # Process oldest first
    
    # Create evolution patterns from collected data
    create_evolution_patterns "$insights_file" "$project" "$language" architectural_patterns[@] feature_evolution[@] technology_adoptions[@] performance_improvements[@] refactoring_events[@]
    
    echo "‚úÖ Comprehensive history analysis completed"
    echo "   üìê Architecture events: ${#architectural_patterns[@]}"
    echo "   üöÄ Feature developments: ${#feature_evolution[@]}"
    echo "   üîß Technology adoptions: ${#technology_adoptions[@]}"
    echo "   ‚ö° Performance improvements: ${#performance_improvements[@]}"
    echo "   üîÑ Refactoring events: ${#refactoring_events[@]}"
}

# Create insight from historical commit
create_historical_insight() {
    local hash="$1"
    local message="$2"
    local category="$3" 
    local significance="$4"
    local insights_file="$5"
    local project="$6"
    local language="$7"
    local author="$8"
    local commit_date="$9"
    
    # Extract meaningful details from the commit
    local files_changed=""
    local lines_changed=""
    local diff_summary=""
    
    if command -v git >/dev/null 2>&1; then
        files_changed=$(git show --name-only --pretty=format: "$hash" 2>/dev/null | head -10 | tr '\n' ', ' | sed 's/,$//' || echo "")
        diff_summary=$(git show --stat --pretty=format: "$hash" 2>/dev/null | tail -n 1 || echo "")
    fi
    
    # Create descriptive name based on commit content
    local insight_name
    case "$category" in
        "architecture")
            insight_name="ArchEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
        "feature")
            insight_name="FeatureEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
        "technology")
            insight_name="TechEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
        "performance")
            insight_name="PerfEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
        "refactoring")
            insight_name="RefactorEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
        *)
            insight_name="CodeEvolution_$(echo "$message" | sed 's/[^a-zA-Z0-9]//g' | cut -c1-15)"
            ;;
    esac
    
    # Ensure unique names
    insight_name="${insight_name}_${hash:0:7}"
    
    local timestamp
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    
    # Add to insights with evolutionary context
    jq --arg name "$insight_name" \
       --arg type "EvolutionPattern" \
       --arg problem "Codebase evolution: $category development" \
       --arg solution "$message" \
       --arg approach "Historical development captured from commit $hash" \
       --arg applicability "Understanding $category evolution in $language projects" \
       --arg author "$author" \
       --arg proj "$project" \
       --arg sig "$significance" \
       --arg timestamp "$timestamp" \
       --arg commit_hash "$hash" \
       --arg commit_date "$commit_date" \
       --arg files "$files_changed" \
       --arg diff "$diff_summary" \
       --arg category "$category" \
       '.insights += [{
           "type": "entity",
           "name": $name,
           "entityType": $type,
           "problem": $problem,
           "solution": $solution,
           "approach": $approach,
           "applicability": $applicability,
           "author": $author,
           "project": $proj,
           "observations": [
               "Evolution: \($solution)",
               "Commit: \($commit_hash) by \($author)",
               "Date: \($commit_date)",
               "Files: \($files)",
               "Stats: \($diff)",
               "Category: \($category)",
               "Significance: \($sig)/10"
           ],
           "significance": ($sig | tonumber),
           "created": $timestamp,
           "metadata": {
               "source": "git-history",
               "commit_hash": $commit_hash,
               "commit_date": $commit_date,
               "evolution_category": $category,
               "files_changed": $files
           }
       }]' "$insights_file" > "$insights_file.tmp" && mv "$insights_file.tmp" "$insights_file"
}

# Create evolution patterns from historical analysis
create_evolution_patterns() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    # This function would analyze the collected patterns to create high-level insights
    # about how the codebase evolved over time
    
    log "Creating evolution patterns from historical analysis..."
    
    # For now, we'll create a summary pattern
    local timestamp
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    
    jq --arg name "CodebaseEvolutionSummary_${project}" \
       --arg type "EvolutionSummary" \
       --arg problem "Understanding comprehensive codebase evolution" \
       --arg solution "Full git history analysis revealing architectural decisions, feature development, and technology adoption patterns" \
       --arg approach "Comprehensive commit analysis with categorization and significance scoring" \
       --arg applicability "Any project requiring deep understanding of codebase evolution and decision history" \
       --arg proj "$project" \
       --arg timestamp "$timestamp" \
       '.insights += [{
           "type": "entity",
           "name": $name,
           "entityType": $type,
           "problem": $problem,
           "solution": $solution,
           "approach": $approach,
           "applicability": $applicability,
           "project": $proj,
           "observations": [
               "Full git history analyzed for comprehensive understanding",
               "Evolution patterns extracted from commit history",
               "Architectural decisions and rationales captured",
               "Technology adoption timeline documented",
               "Feature development progression mapped"
           ],
           "significance": 9,
           "created": $timestamp,
           "metadata": {
               "source": "comprehensive-git-analysis",
               "analysis_type": "full-history"
           }
       }]' "$insights_file" > "$insights_file.tmp" && mv "$insights_file.tmp" "$insights_file"
}

# Conservative commit analysis - only capture major architectural changes
analyze_commits_conservatively() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Analyzing commits conservatively - high threshold for significance..."
    
    local significant_commits=0
    while IFS='|' read -r hash author date message; do
        [[ -z "$message" ]] && continue
        
        # VERY HIGH THRESHOLD: Only capture truly architectural changes
        local should_capture=false
        local category=""
        local significance=0
        
        # Only these patterns are significant enough to auto-capture
        if [[ "$message" =~ ^(feat|refactor):.*implement.*architecture|^(feat|refactor):.*redesign.*system ]]; then
            category="architecture"
            significance=9
            should_capture=true
        elif [[ "$message" =~ ^(feat|refactor):.*agent.*agnostic|^(feat|refactor):.*agnosticity ]]; then
            category="architecture"
            significance=9
            should_capture=true
        elif [[ "$message" =~ ^(perf|feat):.*optimization.*breakthrough|^(perf):.*algorithm.*improvement ]]; then
            category="performance"
            significance=8
            should_capture=true
        elif [[ "$message" =~ ^(feat|refactor):.*state.*management.*pattern|^(feat):.*redux.*implementation ]]; then
            category="state-management"
            significance=8
            should_capture=true
        fi
        
        # Skip everything else - let interactive mode handle nuanced insights
        if [[ "$should_capture" == false ]]; then
            continue
        fi
        
        ((significant_commits++))
        log "Found significant commit: $message (category: $category)"
        
        # Auto-capture clear architectural candidates
        auto_capture_significant_commit "$hash" "$message" "$category" "$significance" "$insights_file" "$project" "$language"
        
    done < "$TMP_DIR/recent_commits.txt"
    
    if [[ $significant_commits -eq 0 ]]; then
        log "No commits met the high significance threshold"
        echo -e "${BLUE}‚ÑπÔ∏è  No significant architectural changes detected in recent commits${NC}"
        echo -e "${BLUE}üí° Use 'ukb --interactive' to manually capture valuable insights${NC}"
    else
        log "Found $significant_commits potentially significant commits"
        echo -e "${YELLOW}‚ö†Ô∏è  Found $significant_commits potentially significant commits${NC}"
        echo -e "${BLUE}üí° Use 'ukb --interactive' to properly capture these insights${NC}"
    fi
}

# Auto-capture significant architectural commits
auto_capture_significant_commit() {
    local hash="$1"
    local message="$2"
    local category="$3"
    local significance="$4"
    local insights_file="$5"
    local project="$6"
    local language="$7"
    
    log "Auto-capturing significant commit: $message"
    
    # Extract meaningful name from commit message
    local pattern_name
    if [[ "$message" =~ implement.*([A-Z][a-z]+.*[A-Z][a-z]+) ]] || [[ "$message" =~ add.*([A-Z][a-z]+.*[A-Z][a-z]+) ]]; then
        # Extract pattern from message using sed instead of BASH_REMATCH
        local extracted_pattern
        extracted_pattern=$(echo "$message" | sed -n 's/.*\(implement\|add\).*\([A-Z][a-z][^[:space:]]*[A-Z][a-z][^[:space:]]*\).*/\2/p' | sed 's/[[:space:]]//g')
        pattern_name="${extracted_pattern}Pattern"
    elif [[ "$category" == "architecture" ]]; then
        pattern_name="ArchitecturalRefactoring_$(date +%Y%m%d)"
    elif [[ "$category" == "performance" ]]; then
        pattern_name="PerformanceOptimization_$(date +%Y%m%d)"
    elif [[ "$category" == "state-management" ]]; then
        pattern_name="StateManagementPattern_$(date +%Y%m%d)"
    else
        pattern_name="${category}Pattern_$(date +%Y%m%d)"
    fi
    
    # Get commit details for context
    local diff_summary=""
    local changed_files=""
    if command -v git >/dev/null 2>&1; then
        diff_summary=$(git show --stat "$hash" 2>/dev/null | tail -n 1 || echo "")
        changed_files=$(git show --name-only --pretty=format: "$hash" 2>/dev/null | head -5 | tr '\n' ', ' | sed 's/,$//' || echo "")
    fi
    
    # Determine problem and solution from commit message and category
    local problem solution approach
    case "$category" in
        "architecture")
            problem="System architecture needed restructuring or new architectural pattern"
            solution="Implemented architectural changes to improve system design"
            approach="$message - $diff_summary"
            ;;
        "performance") 
            problem="Performance bottleneck or optimization opportunity identified"
            solution="Applied performance optimization techniques"
            approach="$message - $diff_summary"
            ;;
        "state-management")
            problem="State management complexity or inefficiency"
            solution="Improved state management pattern or implementation"
            approach="$message - $diff_summary"
            ;;
        *)
            problem="Development challenge requiring systematic solution"
            solution="Implemented structured approach to resolve issue"
            approach="$message - $diff_summary"
            ;;
    esac
    
    # Set technologies based on language and file patterns
    local technologies_array="[]"
    case "$language" in
        "typescript"|"javascript")
            if [[ "$changed_files" =~ react|jsx|tsx ]]; then
                technologies_array='["TypeScript", "React"]'
            elif [[ "$changed_files" =~ node|express ]]; then
                technologies_array='["TypeScript", "Node.js"]'
            else
                technologies_array='["TypeScript"]'
            fi
            ;;
        "python")
            technologies_array='["Python"]'
            ;;
        "rust")
            technologies_array='["Rust"]'
            ;;
        "shell")
            technologies_array='["Bash", "Shell Scripting"]'
            ;;
        *)
            technologies_array="[\"$language\"]"
            ;;
    esac
    
    local code_files_array
    if [[ -n "$changed_files" ]]; then
        code_files_array=$(echo "$changed_files" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R -s 'split("\n") | map(select(length > 0))')
    else
        code_files_array="[]"
    fi
    
    local timestamp
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    local author
    author=$(whoami)
    
    # Add structured pattern to insights file
    jq --arg name "$pattern_name" \
       --arg type "TransferablePattern" \
       --arg problem "$problem" \
       --arg solution "$solution" \
       --arg approach "$approach" \
       --arg applicability "Projects with similar architectural/performance challenges" \
       --argjson technologies "$technologies_array" \
       --argjson code_files "$code_files_array" \
       --arg author "$author" \
       --arg proj "$project" \
       --arg sig "$significance" \
       --arg timestamp "$timestamp" \
       --arg commit_hash "$hash" \
       '.insights += [{
           "type": "entity",
           "name": $name,
           "entityType": $type,
           "problem": $problem,
           "solution": $solution,
           "approach": $approach,
           "applicability": $applicability,
           "technologies": $technologies,
           "author": $author,
           "project": $proj,
           "code_files": $code_files,
           "observations": [
               "Problem: \($problem)",
               "Solution: \($solution)",
               "Approach: \($approach)",
               "Auto-captured from commit: \($commit_hash)",
               "Significance: \($sig)/10",
               "Created: \($timestamp)"
           ],
           "significance": ($sig | tonumber),
           "created": $timestamp,
           "metadata": {
               "source": "auto-commit",
               "commit_hash": $commit_hash
           }
       }]' "$insights_file" > "$insights_file.tmp" && mv "$insights_file.tmp" "$insights_file"
    
    echo -e "${GREEN}‚úÖ Auto-captured: $pattern_name (significance: $significance/10)${NC}"
}

# Analyze code changes for patterns
analyze_code_changes() {
    local insights_file="$1"
    local project="$2"
    local language="$3"
    
    log "Analyzing code changes for architectural patterns..."
    
    # This is where we'd analyze actual code diffs
    # For now, we'll check for common pattern indicators
    
    # Check for significant file changes
    if command -v git >/dev/null 2>&1; then
        local changed_files
        changed_files=$(git diff --name-only HEAD~5..HEAD 2>/dev/null || echo "")
        
        # Look for architectural changes
        if echo "$changed_files" | grep -qE "(store|redux|context|provider|state)" ; then
            log "Detected state management changes"
            # Would add insight about state management refactoring
        fi
        
        if echo "$changed_files" | grep -qE "(api|service|repository|adapter)" ; then
            log "Detected service layer changes"
            # Would add insight about service architecture
        fi
    fi
}

# Create MCP entities with significance
create_mcp_entities() {
    local insights_file="$1"
    
    log "Creating MCP entities from insights..."
    
    # Sort insights by significance
    local sorted_insights
    sorted_insights=$(jq -r '.insights | sort_by(-.significance) | .[] | @json' "$insights_file" 2>/dev/null || echo "")
    
    if [[ -z "$sorted_insights" ]]; then
        log "No insights to process"
        return
    fi
    
    # Process insights in order of significance
    while IFS= read -r entity_json; do
        [[ -z "$entity_json" ]] && continue
        
        local name type observations significance
        name=$(echo "$entity_json" | jq -r '.name')
        type=$(echo "$entity_json" | jq -r '.entityType')
        observations=$(echo "$entity_json" | jq -r '.observations | join("\n")')
        significance=$(echo "$entity_json" | jq -r '.significance // 5')
        
        echo -e "Creating entity: $name ${YELLOW}(significance: $significance/10)${NC}"
        
        # Add to shared memory with significance
        add_to_shared_memory_entity "$name" "$type" "$observations" "$significance"
        
    done <<< "$sorted_insights"
}

# Enhanced entity addition with significance using MCP operations
add_to_shared_memory_entity() {
    local name="$1"
    local type="$2"
    local observations="$3"
    local significance="${4:-5}"
    
    # Convert observations to JSON array for MCP
    local obs_array
    obs_array=$(echo "$observations" | jq -R -s 'split("\n") | map(select(length > 0))')
    
    echo "  Adding to MCP memory: $name"
    
    # Create entity in MCP memory (this will be executed by Claude Code)
    cat > "$TMP_DIR/mcp_entity_${name// /_}.json" << EOF
{
    "entities": [
        {
            "name": "$name",
            "entityType": "$type",
            "observations": $obs_array
        }
    ]
}
EOF
    
    # Also update local shared-memory.json as backup
    # Check if entity already exists locally
    if jq -e --arg name "$name" '.entities[] | select(.name == $name)' "$SHARED_MEMORY" >/dev/null 2>&1; then
        echo "  Entity already exists locally: $name (updating observations)"
        # Add new observations to existing entity
        jq --arg name "$name" \
           --argjson new_obs "$obs_array" \
           --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
           '(.entities[] | select(.name == $name) | .observations) |= (. + $new_obs | unique) |
            .metadata.last_updated = $timestamp' \
           "$SHARED_MEMORY" > "$SHARED_MEMORY.tmp" && mv "$SHARED_MEMORY.tmp" "$SHARED_MEMORY"
    else
        # Create new entity locally
        local timestamp
        timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
        jq --arg name "$name" \
           --arg type "$type" \
           --argjson obs "$obs_array" \
           --arg sig "$significance" \
           --arg timestamp "$timestamp" \
           '.entities += [{
               "type": "entity",
               "name": $name,
               "entityType": $type,
               "observations": $obs,
               "significance": ($sig | tonumber),
               "created": $timestamp
           }] |
           .metadata.total_entities = (.entities | length) |
           .metadata.last_updated = $timestamp' \
           "$SHARED_MEMORY" > "$SHARED_MEMORY.tmp" && mv "$SHARED_MEMORY.tmp" "$SHARED_MEMORY"
    fi
}

# Process MCP entities by creating operations for Claude Code to execute
process_mcp_entities() {
    local mcp_files=("$TMP_DIR"/mcp_entity_*.json)
    
    if [[ ! -e "${mcp_files[0]}" ]]; then
        return  # No MCP files created
    fi
    
    echo -e "${CYAN}üì° Preparing MCP operations for Claude Code...${NC}"
    
    # Create a script that Claude Code should execute to sync with MCP memory
    cat > "$TMP_DIR/mcp_sync_operations.md" << 'EOF'
# MCP Memory Sync Required

The ukb script has captured insights and needs them synced to MCP memory. 
Please execute the following MCP operations in Claude Code:

## Entities to Create

EOF
    
    # Process each MCP entity file
    for mcp_file in "${mcp_files[@]}"; do
        if [[ -f "$mcp_file" ]]; then
            local entity_name entity_type
            entity_name=$(jq -r '.entities[0].name' "$mcp_file")
            entity_type=$(jq -r '.entities[0].entityType' "$mcp_file")
            
            echo "- Entity: **$entity_name** (Type: $entity_type)" >> "$TMP_DIR/mcp_sync_operations.md"
            
            # Append JSON for Claude Code to process
            echo >> "$TMP_DIR/mcp_sync_operations.md"
            echo '```json' >> "$TMP_DIR/mcp_sync_operations.md"
            cat "$mcp_file" >> "$TMP_DIR/mcp_sync_operations.md"
            echo >> "$TMP_DIR/mcp_sync_operations.md"
            echo '```' >> "$TMP_DIR/mcp_sync_operations.md"
            echo >> "$TMP_DIR/mcp_sync_operations.md"
        fi
    done
    
    cat >> "$TMP_DIR/mcp_sync_operations.md" << 'EOF'

## Instructions for Claude Code

1. Use `mcp__memory__create_entities` tool with the JSON data above
2. Create appropriate relations between entities and existing knowledge graph
3. Update the local shared-memory.json to reflect the MCP state

This ensures bidirectional sync between MCP memory and local storage.
EOF
    
    echo -e "${YELLOW}üìã MCP sync instructions created at: $TMP_DIR/mcp_sync_operations.md${NC}"
    echo -e "${YELLOW}ü§ñ Claude Code should process this file to complete the sync${NC}"
    
    # Also show the path for easy access
    echo -e "${BLUE}üìÅ Full path: $TMP_DIR/mcp_sync_operations.md${NC}"
}

# Create relationships with significance weighting
create_project_relationships() {
    log "Creating project relationships..."
    
    local project
    project=$(basename "$PWD")
    
    # Ensure project entity exists with high significance (so it's never filtered out)
    if ! jq -e --arg proj "$project" '.entities[] | select(.name == $proj)' "$SHARED_MEMORY" >/dev/null; then
        add_to_shared_memory_entity "$project" "Project" "Software project: $project" "8"
    fi
    
    # Ensure CodingKnowledge hub exists
    if ! jq -e '.entities[] | select(.name == "CodingKnowledge")' "$SHARED_MEMORY" >/dev/null; then
        add_to_shared_memory_entity "CodingKnowledge" "System" "Central hub for transferable programming patterns and insights" "10"
    fi
    
    # Link insights to project with relationship strength based on significance
    local insights
    insights=$(jq -r '.entities[] | select(.entityType == "CodingInsight" or .entityType == "DeepInsight" or .entityType == "AIAssistedInsight" or .entityType == "TransferablePattern") | "\(.name):\(.significance // 5):\(.entityType)"' "$SHARED_MEMORY" 2>/dev/null || echo "")
    
    while IFS=: read -r insight_name significance entity_type; do
        [[ -z "$insight_name" ]] && continue
        
        # Relationship type based on significance
        local rel_type="contributes to"
        if [[ ${significance:-5} -ge 8 ]]; then
            rel_type="significantly impacts"
        elif [[ ${significance:-5} -ge 6 ]]; then
            rel_type="enhances"
        fi
        
        # Link to project (except for transferable patterns which are cross-project)
        if [[ "$entity_type" != "TransferablePattern" ]]; then
            add_to_shared_memory_relation "$insight_name" "$rel_type" "$project"
        fi
        
        # For high-significance AI-assisted insights, also link to CodingKnowledge hub
        if [[ "$entity_type" == "AIAssistedInsight" ]] && [[ ${significance:-5} -ge 7 ]]; then
            add_to_shared_memory_relation "$insight_name" "exemplifies" "CodingKnowledge"
            log "Linked AI-assisted insight to CodingKnowledge hub: $insight_name"
        fi
        
        # For transferable patterns, link directly to CodingKnowledge hub
        if [[ "$entity_type" == "TransferablePattern" ]]; then
            add_to_shared_memory_relation "CodingKnowledge" "contains" "$insight_name"
            add_to_shared_memory_relation "$insight_name" "derived from" "$project"
            log "Linked transferable pattern to CodingKnowledge hub: $insight_name"
        fi
    done <<< "$insights"
}

# Add relation to shared memory
add_to_shared_memory_relation() {
    local from="$1"
    local relation="$2"
    local to="$3"
    
    # Check if relation already exists
    if jq -e --arg f "$from" --arg r "$relation" --arg t "$to" \
       '.relations[] | select(.from == $f and .relationType == $r and .to == $t)' \
       "$SHARED_MEMORY" >/dev/null 2>&1; then
        return
    fi
    
    # Add relation
    local timestamp
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    jq --arg from "$from" \
       --arg rel "$relation" \
       --arg to "$to" \
       --arg timestamp "$timestamp" \
       '.relations += [{
           "type": "relation",
           "from": $from,
           "relationType": $rel,
           "to": $to,
           "created": $timestamp
       }] |
       .metadata.total_relations = (.relations | length) |
       .metadata.last_updated = $timestamp' \
       "$SHARED_MEMORY" > "$SHARED_MEMORY.tmp" && mv "$SHARED_MEMORY.tmp" "$SHARED_MEMORY"
}

# Show insight summary
show_insight_summary() {
    echo -e "\n${CYAN}üìä Insight Summary${NC}"
    echo -e "${CYAN}==================${NC}"
    
    # Get insights sorted by significance
    local high_sig_insights
    high_sig_insights=$(jq -r '.entities[] | select(.significance >= 7) | "[\(.significance)/10] \(.name)"' "$SHARED_MEMORY" 2>/dev/null | head -5)
    
    if [[ -n "$high_sig_insights" ]]; then
        echo -e "\n${YELLOW}üåü Most Significant Insights:${NC}"
        echo "$high_sig_insights"
    fi
    
    # Show distribution
    local total_insights sig_high sig_med sig_low
    total_insights=$(jq '[.entities[] | select(.entityType == "CodingInsight" or .entityType == "DeepInsight")] | length' "$SHARED_MEMORY")
    sig_high=$(jq '[.entities[] | select(.significance >= 7)] | length' "$SHARED_MEMORY" 2>/dev/null || echo "0")
    sig_med=$(jq '[.entities[] | select(.significance >= 4 and .significance < 7)] | length' "$SHARED_MEMORY" 2>/dev/null || echo "0")
    sig_low=$(jq '[.entities[] | select(.significance < 4)] | length' "$SHARED_MEMORY" 2>/dev/null || echo "0")
    
    echo -e "\n${YELLOW}üìà Significance Distribution:${NC}"
    echo -e "  High (7-10):   $sig_high insights"
    echo -e "  Medium (4-6):  $sig_med insights"
    echo -e "  Low (1-3):     $sig_low insights"
}

# Analyze current session
analyze_session() {
    log "Analyzing current session..."
    
    local session_file="$TMP_DIR/session_analysis.json"
    
    # Debug: Check if TMP_DIR exists
    if [[ ! -d "$TMP_DIR" ]]; then
        mkdir -p "$TMP_DIR"
    fi
    
    # Get current project context first (before using project_name)
    local project_name
    # Use git remote to determine actual project name if available
    if git remote -v 2>/dev/null | grep -q origin; then
        # Extract project name from git remote URL
        project_name=$(git remote get-url origin 2>/dev/null | sed -E 's/.*[:/]([^/]+)\/[^/]+\.git$/\1/' || basename "$PWD")
        # Handle common cases
        case "$project_name" in
            "q284340"|"user"|".")
                project_name=$(basename "$PWD")
                ;;
        esac
    else
        project_name=$(basename "$PWD")
    fi
    
    # Get git status and recent commits
    cd "$PWD" 2>/dev/null || cd "$HOME"
    
    # Determine commit range based on mode
    if [[ "$FULL_HISTORY_MODE" == true ]]; then
        # Full history analysis
        if [[ -n "$HISTORY_DEPTH" ]]; then
            git log --oneline -"$HISTORY_DEPTH" --pretty=format:'%h|%an|%ad|%s' --date=iso > "$TMP_DIR/recent_commits.txt" 2>/dev/null || true
            echo "üìä FULL HISTORY: Analyzing last $HISTORY_DEPTH commits"
        else
            git log --oneline --pretty=format:'%h|%an|%ad|%s' --date=iso > "$TMP_DIR/recent_commits.txt" 2>/dev/null || true
            local total_commits
            total_commits=$(wc -l < "$TMP_DIR/recent_commits.txt" 2>/dev/null || echo 0)
            echo "üìä FULL HISTORY: Analyzing entire git history ($total_commits commits)"
        fi
        
        # In full history mode, we process everything regardless of previous state
        echo "üîç Full history mode: Processing all commits for comprehensive understanding"
    else
        # Incremental analysis (default behavior)
        local last_commit
        last_commit=$(get_last_analyzed_commit "$project_name")
        
        if [[ "$last_commit" != "never" ]] && git rev-parse --verify "$last_commit" >/dev/null 2>&1; then
            # Get commits since last analyzed commit
            git log --oneline --pretty=format:'%h|%an|%ad|%s' --date=iso "${last_commit}..HEAD" > "$TMP_DIR/recent_commits.txt" 2>/dev/null || true
            echo "üìä INCREMENTAL: Analyzing $(wc -l < "$TMP_DIR/recent_commits.txt" 2>/dev/null || echo 0) new commits since $last_commit"
        else
            # First run or invalid last commit - analyze last 10 commits
            git log --oneline -10 --pretty=format:'%h|%an|%ad|%s' --date=iso > "$TMP_DIR/recent_commits.txt" 2>/dev/null || true
            echo "üìä FIRST RUN: Examining last 10 commits"
        fi
        
        # Update last analyzed commit to current HEAD (only in incremental mode)
        local current_head
        current_head=$(git rev-parse HEAD 2>/dev/null || echo "unknown")
        if [[ "$current_head" != "unknown" ]]; then
            update_last_analyzed_commit "$project_name" "$current_head"
        fi
    fi
    
    # Determine main language
    local main_language="unknown"
    if [[ -f "package.json" ]]; then
        main_language="typescript"
    elif [[ -f "Cargo.toml" ]]; then
        main_language="rust"
    elif [[ -f "go.mod" ]]; then
        main_language="go"
    elif [[ -f "pyproject.toml" ]] || [[ -f "requirements.txt" ]]; then
        main_language="python"
    elif [[ "$project_name" == "knowledge-management" ]] || [[ "$project_name" == "Claude" ]]; then
        main_language="shell"
    fi
    
    # Create session analysis
    cat > "$session_file" << EOF
{
  "session_id": "session_$(date +%Y%m%d_%H%M%S)",
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "project": "$project_name",
  "working_directory": "$PWD",
  "main_language": "$main_language",
  "mode": "$([[ "$INTERACTIVE_MODE" == true ]] && echo "interactive" || echo "automatic")"
}
EOF
    
    echo "$session_file"
}

# Main execution
main() {
    echo -e "${PURPLE}üß† UKB - Update Knowledge Base v3.0${NC}"
    echo -e "${PURPLE}======================================${NC}"
    
    # Check if schema migration is needed
    if needs_schema_migration; then
        echo -e "${YELLOW}üîÑ Schema migration needed - upgrading to v2.0.0...${NC}"
        if command -v node >/dev/null 2>&1 && [[ -f "$CLAUDE_REPO/knowledge-management/scripts/migrate-entities.js" ]]; then
            node "$CLAUDE_REPO/knowledge-management/scripts/migrate-entities.js"
            echo -e "${GREEN}‚úÖ Schema migration completed${NC}"
        else
            echo -e "${RED}‚ö†Ô∏è  Schema migration required but migrate-entities.js not available${NC}"
        fi
    fi
    
    # Analyze session
    local session_file
    session_file=$(analyze_session)
    
    # Check if session file exists
    if [[ ! -f "$session_file" ]]; then
        error_exit "Failed to create session analysis"
    fi
    
    local project_name main_language
    project_name=$(jq -r '.project' "$session_file")
    main_language=$(jq -r '.main_language' "$session_file")
    
    # Initialize processing state for this project
    init_processing_state "$project_name"
    
    # Handle upgrade mode
    if [[ "$UPGRADE_MODE" == true ]]; then
        echo -e "${CYAN}üîÑ Running in UPGRADE mode - migrating to enhanced schema${NC}"
        if command -v node >/dev/null 2>&1 && [[ -f "$CLAUDE_REPO/knowledge-management/scripts/migrate-entities.js" ]]; then
            node "$CLAUDE_REPO/knowledge-management/scripts/migrate-entities.js"
            echo -e "${GREEN}‚úÖ Schema upgrade completed${NC}"
            exit 0
        else
            error_exit "Node.js or migrate-entities.js not available for schema upgrade"
        fi
    fi
    
    # Process insights based on mode
    local insights_file="$TMP_DIR/insights.json"
    
    if [[ "$INTERACTIVE_MODE" == true ]]; then
        echo -e "${CYAN}üéØ Running in INTERACTIVE mode${NC}"
        echo '{"insights": [], "entities": [], "relations": []}' > "$insights_file"
        
        # Capture deep insight interactively
        capture_interactive_insight "$insights_file" "$project_name" "$main_language"
        
        # Ask if user wants to add more
        while true; do
            echo -e "\n${YELLOW}Add another insight? (y/n)${NC}"
            read -r -n 1 answer
            echo
            if [[ "$answer" != "y" ]]; then
                break
            fi
            capture_interactive_insight "$insights_file" "$project_name" "$main_language"
        done
    elif [[ "$AGENT_MODE" == true ]]; then
        echo -e "${CYAN}ü§ñ Running in AGENT mode (semantic analysis)${NC}"
        extract_agent_insights "$insights_file" "$project_name" "$main_language"
    else
        echo -e "${CYAN}ü§ñ Running in AUTOMATIC mode${NC}"
        extract_auto_insights "$insights_file" "$project_name" "$main_language"
    fi
    
    # Create entities and relationships
    if [[ -f "$insights_file" ]] && [[ $(jq '.insights | length' "$insights_file") -gt 0 ]]; then
        create_mcp_entities "$insights_file"
        create_project_relationships
        
        # Process MCP entities if any were created
        process_mcp_entities
    else
        echo -e "${YELLOW}‚ö†Ô∏è  No insights captured${NC}"
    fi
    
    # Update metadata
    local timestamp contributor
    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    contributor=$(whoami)
    jq --arg contributor "$contributor" \
       --arg timestamp "$timestamp" \
       --arg mode "$([[ "$INTERACTIVE_MODE" == true ]] && echo "interactive" || echo "automatic")" \
       '.metadata.contributors |= (. + [$contributor] | unique) |
        .metadata.last_updated = $timestamp |
        .metadata.last_mode = $mode' \
       "$SHARED_MEMORY" > "$SHARED_MEMORY.tmp" && mv "$SHARED_MEMORY.tmp" "$SHARED_MEMORY"
    
    # Show summary
    show_insight_summary
    
    # Final stats
    local entity_count relation_count
    entity_count=$(jq '.entities | length' "$SHARED_MEMORY")
    relation_count=$(jq '.relations | length' "$SHARED_MEMORY")
    
    echo -e "\n${GREEN}‚úÖ Knowledge base updated successfully!${NC}"
    echo -e "${GREEN}üìä Total entities: $entity_count${NC}"
    echo -e "${GREEN}üîó Total relations: $relation_count${NC}"
    echo -e "${GREEN}üíæ Shared memory: $SHARED_MEMORY${NC}"
    
    log "UKB completed successfully"
}

# Parse arguments and run
parse_args "$@"
main