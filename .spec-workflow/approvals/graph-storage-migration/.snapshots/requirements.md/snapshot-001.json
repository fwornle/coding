{
  "id": "snapshot_1761037028873_crxtt367e",
  "approvalId": "approval_1761037028863_2eu403oyj",
  "approvalTitle": "Graph Storage Migration - Requirements Document",
  "version": 1,
  "timestamp": "2025-10-21T08:57:08.873Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses the migration of the knowledge management storage architecture from a SQLite-based relational model to a graph-based architecture. The current implementation suffers from **impedance mismatch** - knowledge is naturally a graph (entities with rich relationships), but SQLite forces this into tables, resulting in complex queries, poor traversal performance, and schema rigidity.\n\n### Problem Statement\n\nThe `coding` project currently uses 6+ persistence mechanisms, creating unnecessary complexity:\n\n1. **SQLite**: Stores knowledge entities and relationships in tables, requiring expensive JOINs for graph traversal\n2. **MCP Memory server**: Anthropic's memory service creates agent lock-in (only works with Claude Code)\n3. **Qdrant**: Vector search (works well, no changes needed)\n4. **JSON files**: Git-tracked knowledge exports (manual process)\n5. **Graphology**: In-memory graph library (underutilized)\n6. **DuckDB**: Never implemented (was placeholder)\n\n### Value Proposition\n\nBy migrating to a graph-first architecture:\n\n- **Developer Productivity**: 10x faster knowledge queries through index-free adjacency\n- **Agent Agnostic**: Remove MCP Memory dependency, enabling any AI assistant (Claude Code, Copilot, Cursor, Aider)\n- **Simplified Architecture**: Reduce from 6 mechanisms to 4 well-defined systems\n- **Better Data Model**: Graph structure matches the natural shape of knowledge\n- **Maintainability**: Auto-sync to JSON eliminates manual export processes\n\n## Alignment with Product Vision\n\nThis migration directly supports the core product vision:\n\n### 1. Agent-Agnostic Design (Product Vision Â§1)\n- **Current State**: MCP Memory server locks us into Claude Code\n- **After Migration**: Direct graph database access works with any AI agent\n- **Benefit**: Developers can switch between Claude Code, Copilot, Cursor without losing knowledge\n\n### 2. Comprehensive Knowledge Management System (Product Vision Â§2)\n- **Current State**: SQLite's relational model creates impedance mismatch for graph data\n- **After Migration**: Native graph storage enables natural representation of entities and relationships\n- **Benefit**: Faster queries, richer relationships, flexible schema evolution\n\n### 3. Technical Standards (Tech Standards Â§2.3)\n- **Configuration-Driven Architecture**: Maintains existing config patterns\n- **Performance Standards**: <100ms for knowledge operations (graph databases excel here)\n- **Agent-Agnostic Design Pattern**: Removes MCP-specific dependencies\n\n### 4. Product Principle: Universal Compatibility (Product Â§4.2)\n- **Current**: MCP Memory only works with Claude Code\n- **After**: Graph database accessible from any development environment\n- **Benefit**: True universal compatibility across all AI coding assistants\n\n## Requirements\n\n### Requirement 1: Graph Database Implementation\n\n**User Story**: As a developer using any AI coding assistant, I want knowledge stored in a graph database so that relationship queries are fast and natural.\n\n#### Acceptance Criteria\n\n1. WHEN the system initializes THEN it SHALL create a Graphology graph instance with Level persistence\n2. WHEN an entity is stored THEN the system SHALL create a graph node with all entity attributes\n3. WHEN a relationship is stored THEN the system SHALL create a graph edge with relationship metadata\n4. WHEN querying for related entities THEN the system SHALL traverse the graph using index-free adjacency\n5. IF the database file doesn't exist THEN the system SHALL create it automatically\n6. WHEN the graph is modified THEN the system SHALL persist changes to Level storage within 1 second\n\n**Technical Constraints**:\n- Must use Graphology (already installed: v0.25.4)\n- Must use Level for persistence (v10.0.0, compatible with Node.js 24+)\n- Must NOT use LevelGraph (incompatible with Node.js 24)\n- Database location: `.data/knowledge-graph/`\n\n### Requirement 2: Remove MCP Memory Server\n\n**User Story**: As a developer who might use different AI assistants, I want the knowledge system to work without agent-specific dependencies so that I can switch tools freely.\n\n#### Acceptance Criteria\n\n1. WHEN the system starts THEN it SHALL NOT require MCP Memory server\n2. WHEN knowledge is accessed THEN the system SHALL use the graph database directly\n3. WHEN migrating THEN the system SHALL preserve all entities and relationships from MCP Memory\n4. IF MCP Memory was previously used THEN the migration script SHALL extract all data\n5. WHEN cleanup is complete THEN `claude-code-mcp.json` SHALL NOT contain the `memory` server entry\n\n**Migration Path**:\n- Export all data from MCP Memory before removal\n- Verify data integrity after migration\n- Remove MCP Memory from service configuration\n- Update documentation to reflect removal\n\n### Requirement 3: Maintain Vector Search with Qdrant\n\n**User Story**: As a developer searching for knowledge, I want semantic search to continue working so that I can find relevant information by meaning.\n\n#### Acceptance Criteria\n\n1. WHEN semantic search is performed THEN Qdrant collections SHALL remain unchanged\n2. WHEN an entity is stored THEN embeddings SHALL still be generated and stored in Qdrant\n3. WHEN searching THEN the system SHALL return results from both graph and vector search\n4. IF Qdrant is unavailable THEN the system SHALL fall back to graph-only search\n5. WHEN migrating THEN all existing vectors SHALL remain accessible\n\n**Qdrant Collections** (unchanged):\n- `knowledge_patterns` (1536-dim, OpenAI embeddings)\n- `knowledge_patterns_small` (384-dim, nomic-embed-text)\n- `trajectory_analysis` (384-dim)\n- `session_memory` (384-dim)\n\n### Requirement 4: Auto-Sync to JSON Files\n\n**User Story**: As a team member, I want knowledge automatically exported to JSON files so that it's version controlled and shareable.\n\n#### Acceptance Criteria\n\n1. WHEN a graph modification occurs THEN the system SHALL schedule a JSON export\n2. WHEN multiple modifications occur within 5 seconds THEN the system SHALL batch them into one export\n3. WHEN exporting THEN the system SHALL write to `shared-memory-{team}.json` files\n4. WHEN export completes THEN the JSON file SHALL contain all entities and relationships for that team\n5. IF export fails THEN the system SHALL retry up to 3 times with exponential backoff\n6. WHEN export succeeds THEN the system SHALL emit an event for potential git auto-commit\n\n**Export Format** (maintain compatibility):\n```json\n{\n  \"entities\": [\n    {\n      \"name\": \"PatternName\",\n      \"entityType\": \"TechnicalPattern\",\n      \"observations\": [...],\n      \"significance\": 8,\n      \"tags\": [\"authentication\", \"security\"]\n    }\n  ],\n  \"relations\": [\n    {\n      \"from\": \"Pattern:JWT\",\n      \"to\": \"Problem:StatelessAuth\",\n      \"type\": \"solves\",\n      \"confidence\": 0.9\n    }\n  ],\n  \"metadata\": {\n    \"last_updated\": \"2025-10-21T10:30:00Z\",\n    \"team\": \"coding\",\n    \"entity_count\": 47,\n    \"relation_count\": 123\n  }\n}\n```\n\n### Requirement 5: Preserve Analytics Capabilities\n\n**User Story**: As a project manager, I want budget tracking and session analytics to continue working so that I can monitor development metrics.\n\n#### Acceptance Criteria\n\n1. WHEN migrating THEN SQLite SHALL retain only analytics tables (budget_events, session_metrics, embedding_cache)\n2. WHEN knowledge tables are removed THEN analytics queries SHALL continue working\n3. WHEN storing budget events THEN the system SHALL use SQLite as before\n4. WHEN querying session metrics THEN performance SHALL be equivalent to current implementation\n5. IF analytics are queried THEN response time SHALL be <50ms for standard queries\n\n**SQLite Tables to KEEP**:\n- âœ… `budget_events` (cost tracking)\n- âœ… `session_metrics` (session analytics)\n- âœ… `embedding_cache` (embedding reuse)\n\n**SQLite Tables to REMOVE**:\n- ðŸ”´ `knowledge_extractions` (migrate to graph nodes)\n- ðŸ”´ `knowledge_relations` (migrate to graph edges)\n- ðŸ”´ `trajectory_history` (migrate to graph)\n\n### Requirement 6: Migration Data Integrity\n\n**User Story**: As a developer with existing knowledge, I want all my current knowledge preserved during migration so that I don't lose accumulated insights.\n\n#### Acceptance Criteria\n\n1. WHEN migration starts THEN the system SHALL export all current SQLite knowledge to backup files\n2. WHEN migrating entities THEN the system SHALL preserve all attributes (name, type, observations, confidence, tags, team)\n3. WHEN migrating relationships THEN the system SHALL preserve all edges with metadata\n4. WHEN migration completes THEN the system SHALL verify entity count matches source count\n5. IF verification fails THEN the system SHALL restore from backup and abort migration\n6. WHEN verification succeeds THEN the system SHALL create a migration report with statistics\n\n**Verification Checks**:\n- Entity count: SQLite rows = Graph nodes\n- Relationship count: SQLite rows = Graph edges\n- Random sampling: 10 entities checked for attribute integrity\n- Relationship sampling: 10 edges checked for metadata integrity\n\n### Requirement 7: Rollback Capability\n\n**User Story**: As a system administrator, I want the ability to rollback if migration fails so that the system remains operational.\n\n#### Acceptance Criteria\n\n1. WHEN migration starts THEN the system SHALL create timestamped backups of all data\n2. IF migration fails THEN the system SHALL provide a rollback command\n3. WHEN rollback is executed THEN the system SHALL restore SQLite from backup\n4. WHEN rollback completes THEN all services SHALL use the restored SQLite database\n5. IF rollback fails THEN the system SHALL provide manual recovery instructions\n6. WHEN rollback succeeds THEN the system SHALL be in the exact pre-migration state\n\n**Backup Strategy**:\n- SQLite: Copy `.data/knowledge.db` â†’ `.data/backups/knowledge.db.{timestamp}`\n- JSON: Copy `shared-memory-*.json` â†’ `.data/backups/shared-memory-*.json.{timestamp}`\n- Config: Copy service configs â†’ `.data/backups/config.{timestamp}/`\n\n### Requirement 8: Performance Standards\n\n**User Story**: As a developer using the knowledge system, I want fast query responses so that knowledge retrieval doesn't slow down my workflow.\n\n#### Acceptance Criteria\n\n1. WHEN querying a single entity THEN response time SHALL be <10ms\n2. WHEN traversing 2-hop relationships THEN response time SHALL be <50ms\n3. WHEN traversing 3+ hop relationships THEN response time SHALL be <100ms\n4. WHEN storing an entity THEN persistence SHALL complete within 100ms\n5. WHEN exporting to JSON THEN the operation SHALL complete within 500ms for <1000 entities\n6. IF performance degrades THEN the system SHALL log performance metrics for analysis\n\n**Performance Benchmarks**:\n- **Current (SQLite)**: 2-hop relationship query = ~200ms (3 JOINs + recursive CTE)\n- **Target (Graph)**: 2-hop relationship query = ~50ms (index-free adjacency)\n- **Expected Improvement**: 4x faster for relationship queries\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n\n#### Single Responsibility Principle\n- **GraphDatabaseService**: Manages graph database operations (Graphology + Level)\n- **GraphMigrationService**: Handles one-time migration from SQLite to graph\n- **GraphExportService**: Auto-syncs graph to JSON files\n- **KnowledgeStorageService**: Orchestrates graph + Qdrant operations (existing, will be updated)\n\n#### Modular Design\n- Graph database module is standalone and reusable\n- Export service can work with any graph implementation\n- Migration service is one-time use, isolated from core functionality\n- All services communicate through well-defined events\n\n#### Dependency Management\n- Graphology: Already installed (v0.25.4)\n- Level: Will be added (v10.0.0)\n- LevelGraph: Will NOT be used (incompatibility confirmed)\n- No new external dependencies beyond Level\n\n#### Clear Interfaces\n```javascript\n// GraphDatabaseService interface\nclass GraphDatabaseService {\n  async initialize()\n  async storeEntity(entity, options)\n  async getEntity(name, team)\n  async storeRelationship(from, to, type, metadata)\n  async findRelated(entityName, depth, filter)\n  async exportToJSON(team)\n  async getHealth()\n  async close()\n}\n```\n\n### Performance\n\n**Response Time Requirements**:\n- Single entity retrieval: <10ms (p95)\n- 2-hop traversal: <50ms (p95)\n- 3-hop traversal: <100ms (p95)\n- Entity storage: <100ms (p95)\n- JSON export: <500ms for <1000 entities\n\n**Resource Usage**:\n- Memory: <150MB for graph database (up from <100MB, acceptable for in-memory graph)\n- Disk: Efficient binary storage via Level (estimated 2x compression vs JSON)\n- CPU: <5% during normal operation, <20% during intensive queries\n\n**Scalability**:\n- Support for 10,000+ entities without degradation\n- Support for 50,000+ relationships without degradation\n- Graceful performance degradation beyond these limits\n\n### Security\n\n**Data Protection**:\n- Graph database files must have appropriate file permissions (600)\n- JSON exports maintain existing secret redaction patterns\n- No sensitive data in graph database file names or paths\n\n**Access Control**:\n- Team-based isolation (entities scoped by team attribute)\n- No cross-team data leakage\n- Migration preserves team boundaries\n\n**Audit Trail**:\n- Migration creates detailed logs of all operations\n- Export operations logged with timestamps\n- Rollback operations fully auditable\n\n### Reliability\n\n**Data Integrity**:\n- Graph modifications are atomic (node + persistence)\n- Level provides ACID guarantees for persistence\n- JSON exports maintain referential integrity (no dangling relationship references)\n\n**Fault Tolerance**:\n- Graceful degradation if Level is unavailable (in-memory only mode with warning)\n- Retry logic for JSON exports (3 attempts with exponential backoff)\n- Comprehensive error handling with context preservation\n\n**Monitoring**:\n- Health checks for graph database service\n- Performance metrics logged for all operations\n- Export success/failure rates tracked\n\n### Usability\n\n**Migration Process**:\n- Single command migration with progress reporting\n- Clear success/failure messages\n- Automatic verification with detailed report\n- Simple rollback command if needed\n\n**Developer Experience**:\n- Consistent API with existing KnowledgeStorageService\n- Backward compatible JSON export format\n- No changes required to VKB visualization\n- Comprehensive logging for debugging\n\n**Documentation**:\n- Migration guide with step-by-step instructions\n- Rollback procedures clearly documented\n- Architecture diagrams showing before/after state\n- Performance comparison benchmarks\n\n### Compatibility\n\n**Node.js Version**:\n- Must work with Node.js 24.5.0 (current environment)\n- Target compatibility: Node.js 18+\n- No native dependencies beyond Level (which is well-maintained)\n\n**Agent Agnostic**:\n- Works with Claude Code (MCP integration)\n- Works with GitHub Copilot (HTTP API)\n- Works with Cursor (HTTP API)\n- Works with Aider (direct file access)\n- Works with any tool that can access Node.js modules\n\n**Operating Systems**:\n- macOS (primary development environment)\n- Linux (server deployments)\n- Windows (via WSL2, best effort)\n\n### Maintainability\n\n**Code Quality**:\n- ESLint compliance with project standards\n- JSDoc comments for all public methods\n- Comprehensive error messages with context\n- Structured logging with appropriate log levels\n\n**Testing**:\n- Unit tests for GraphDatabaseService (>90% coverage)\n- Integration tests for migration process\n- Performance tests validating <100ms requirement\n- End-to-end tests for auto-sync functionality\n\n**Documentation**:\n- Architecture decision records (ADRs) for key choices\n- API documentation for all public interfaces\n- Migration runbook for operations team\n- Troubleshooting guide for common issues\n\n## Success Criteria\n\nThis specification will be considered successfully implemented when:\n\n1. âœ… All knowledge entities are stored in graph database with full attribute preservation\n2. âœ… All relationships are stored as graph edges with metadata\n3. âœ… Relationship queries are 4x faster than current SQLite implementation\n4. âœ… MCP Memory server is removed from configuration\n5. âœ… Auto-sync to JSON files works reliably with <5s latency\n6. âœ… VKB visualization works with new graph database\n7. âœ… Zero knowledge loss during migration (verified by entity/relationship counts)\n8. âœ… Rollback capability tested and documented\n9. âœ… All acceptance criteria met\n10. âœ… Performance benchmarks achieved (p95 <100ms for 3-hop queries)\n\n## Out of Scope\n\nThe following are explicitly **not** part of this specification:\n\n- âŒ Changes to Qdrant vector search functionality\n- âŒ Modifications to VKB visualization UI\n- âŒ Changes to UKB command-line interface\n- âŒ Alterations to secret redaction patterns\n- âŒ Changes to SQLite analytics tables (budget_events, session_metrics)\n- âŒ Modifications to live session logging system\n- âŒ Integration with external graph databases (Neo4j, ArangoDB)\n- âŒ Graph visualization enhancements beyond current VKB capabilities\n\n## Dependencies\n\n**External Libraries**:\n- Graphology v0.25.4 (already installed)\n- Level v10.0.0 (new dependency, to be added)\n- graphology-utils (already installed)\n\n**Internal Dependencies**:\n- KnowledgeStorageService (will be updated)\n- DatabaseManager (will be updated)\n- VKB server (no changes, reads JSON files)\n- UKB command (no changes, uses KnowledgeStorageService)\n\n**Data Dependencies**:\n- Existing SQLite database: `.data/knowledge.db`\n- Existing JSON files: `shared-memory-*.json`\n- Qdrant collections (read-only dependency)\n\n## Risk Mitigation\n\n**Risk 1: Data Loss During Migration**\n- **Mitigation**: Comprehensive backup before migration, verification checks, rollback capability\n- **Probability**: Low (with proper testing)\n- **Impact**: High\n\n**Risk 2: Performance Degradation**\n- **Mitigation**: Performance benchmarks, load testing, comparison with SQLite baseline\n- **Probability**: Low (graph databases excel at relationship queries)\n- **Impact**: Medium\n\n**Risk 3: Level Compatibility Issues**\n- **Mitigation**: Level v10 explicitly supports Node.js 18+, verified in testing\n- **Probability**: Very Low\n- **Impact**: High\n\n**Risk 4: JSON Export Failure**\n- **Mitigation**: Retry logic, error handling, manual export command as fallback\n- **Probability**: Low\n- **Impact**: Medium (doesn't affect core functionality)\n\n**Risk 5: Team Resistance to New Architecture**\n- **Mitigation**: Clear documentation, performance improvements, gradual rollout option\n- **Probability**: Medium\n- **Impact**: Low (technical benefits are clear)\n\n---\n\n**Document Status**: âœ… Ready for Review\n**Next Phase**: Design Document (after approval)\n**Estimated Implementation Time**: 12-16 hours\n**Breaking Changes**: None (backward compatible JSON format, API consistency)\n",
  "fileStats": {
    "size": 18761,
    "lines": 456,
    "lastModified": "2025-10-21T08:57:00.723Z"
  },
  "comments": []
}